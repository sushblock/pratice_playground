{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237a9ffd",
   "metadata": {},
   "source": [
    "1. Define tools and model\n",
    "In this example, weâ€™ll use the OpenAI GPT-4.1-mini model and define tools for addition, multiplication, and division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a489888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just imports\n",
    "import os\n",
    "import getpass\n",
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import add_messages\n",
    "from langchain.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    ToolCall,\n",
    ")\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.func import entrypoint, task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d4c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    # Prompt user for API key securely\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "# Initialize the chat model with specified parameters\n",
    "model = init_chat_model(\"gpt-4.1-mini\", temperature=0.5, max_retries=3, timeout=60, max_tokens=1000, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb0aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082aa759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "# Create a mapping of tool names to tool instances\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6926ac5",
   "metadata": {},
   "source": [
    "2. Define model node\n",
    "The model node is used to call the LLM and decide whether to call a tool or not.\n",
    "The @task decorator marks a function as a task that can be executed as part of the agent. Tasks can be called synchronously or asynchronously within your entrypoint function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e0acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def call_llm(messages: list[BaseMessage]):\n",
    "    \"\"\"LLM decides whether to call a tool or not.\"\"\"\n",
    "    # Prepare the messages for the LLM invocation\n",
    "    return model_with_tools.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "            )\n",
    "        ]\n",
    "        + messages\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c78b10",
   "metadata": {},
   "source": [
    "3. Define tool node\n",
    "The tool node is used to call the tools and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7927853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def call_tool(tool_call: ToolCall):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    return tool.invoke(tool_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f48ca2",
   "metadata": {},
   "source": [
    "4. Define agent\n",
    "The agent is built using the @entrypoint function.\n",
    "In the Functional API, instead of defining nodes and edges explicitly, you write standard control flow logic (loops, conditionals) within a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f976b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@entrypoint()\n",
    "def agent(messages: list[BaseMessage]):\n",
    "    # Initial LLM call\n",
    "    model_response = call_llm(messages).result()\n",
    "    # Loop until there are no more tool calls\n",
    "    while True:\n",
    "\n",
    "        if not model_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_result_futures = [\n",
    "            call_tool(tool_call) for tool_call in model_response.tool_calls\n",
    "        ]\n",
    "        # Gather tool results\n",
    "        tool_results = [fut.result() for fut in tool_result_futures]\n",
    "        # Add model response and tool results to messages\n",
    "        messages = add_messages(messages, [model_response, *tool_results])\n",
    "        # Update the model response with new tool results\n",
    "        model_response = call_llm(messages).result()\n",
    "\n",
    "    # Final addition of the model response\n",
    "    messages = add_messages(messages, model_response)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2376d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_llm': AIMessage(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--f0ff335f-76f5-4c52-9174-706e167565be', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_XuM4XuZlN7xAC7Kxm6PBsJZ7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 162, 'output_tokens': 17, 'total_tokens': 179, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}\n",
      "\n",
      "\n",
      "{'call_tool': ToolMessage(content='7', name='add', id='4987895e-fe42-4d58-ab75-f449c1e42746', tool_call_id='call_XuM4XuZlN7xAC7Kxm6PBsJZ7')}\n",
      "\n",
      "\n",
      "{'call_llm': AIMessage(content='3 plus 4 is 7.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--c47db8c4-db83-49ba-9240-085f8096c67d', usage_metadata={'input_tokens': 187, 'output_tokens': 9, 'total_tokens': 196, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}\n",
      "\n",
      "\n",
      "{'agent': [HumanMessage(content='Add 3 and 4.', additional_kwargs={}, response_metadata={}, id='5d4d72af-357c-46b3-9351-46199342d4cd'), AIMessage(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--f0ff335f-76f5-4c52-9174-706e167565be', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_XuM4XuZlN7xAC7Kxm6PBsJZ7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 162, 'output_tokens': 17, 'total_tokens': 179, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='7', name='add', id='4987895e-fe42-4d58-ab75-f449c1e42746', tool_call_id='call_XuM4XuZlN7xAC7Kxm6PBsJZ7'), AIMessage(content='3 plus 4 is 7.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_9766e549b2', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--c47db8c4-db83-49ba-9240-085f8096c67d', usage_metadata={'input_tokens': 187, 'output_tokens': 9, 'total_tokens': 196, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke the agent with a sample input\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "for chunk in agent.stream(messages, stream_mode=\"updates\"):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
